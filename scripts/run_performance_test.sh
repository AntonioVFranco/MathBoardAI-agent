#!/bin/bash # Performance Test Orchestration Script for MathBoardAI Agent # # This script orchestrates the complete performance testing suite including: # - Application startup # - Resource monitoring # - Locust load testing # - Results collection and analysis # # Author: MathBoardAI Agent Team # Task ID: TEST-003 set -e # Exit on any error # Script configuration SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)" PROJECT_ROOT="$(dirname "$SCRIPT_DIR")" PERFORMANCE_DIR="$PROJECT_ROOT/tests/performance" RESULTS_DIR="$PROJECT_ROOT/test_results/performance" # Default test parameters DEFAULT_USERS=50 DEFAULT_SPAWN_RATE=5 DEFAULT_DURATION=180 # 3 minutes DEFAULT_HOST="http://localhost:7860" DEFAULT_CONTAINER_NAME="math-ai-agent-dev" # Test configuration USERS=${USERS:-$DEFAULT_USERS} SPAWN_RATE=${SPAWN_RATE:-$DEFAULT_SPAWN_RATE} DURATION=${DURATION:-$DEFAULT_DURATION} HOST=${HOST:-$DEFAULT_HOST} CONTAINER_NAME=${CONTAINER_NAME:-$DEFAULT_CONTAINER_NAME} # File paths LOCUST_FILE="$PERFORMANCE_DIR/locustfile.py" RESOURCE_MONITOR="$PERFORMANCE_DIR/resource_monitor.py" RESOURCE_LOG="$RESULTS_DIR/resource_log.csv" LOCUST_LOG="$RESULTS_DIR/locust_log.csv" PERFORMANCE_REPORT="$RESULTS_DIR/performance_report.html" # Process IDs for cleanup DOCKER_PID="" RESOURCE_MONITOR_PID="" LOCUST_PID="" # Colors for output RED='\033[0;31m' GREEN='\033[0;32m' YELLOW='\033[1;33m' BLUE='\033[0;34m' NC='\033[0m' # No Color # Logging functions log_info() { echo -e "${BLUE}[INFO]${NC} $1" } log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1" } log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1" } log_error() { echo -e "${RED}[ERROR]${NC} $1" } # Cleanup function cleanup() { log_info "Cleaning up processes..." # Stop Locust if [ ! -z "$LOCUST_PID" ] && kill -0 "$LOCUST_PID" 2>/dev/null; then log_info "Stopping Locust (PID: $LOCUST_PID)..." kill -TERM "$LOCUST_PID" 2>/dev/null || true wait "$LOCUST_PID" 2>/dev/null || true fi # Stop Resource Monitor if [ ! -z "$RESOURCE_MONITOR_PID" ] && kill -0 "$RESOURCE_MONITOR_PID" 2>/dev/null; then log_info "Stopping Resource Monitor (PID: $RESOURCE_MONITOR_PID)..." kill -TERM "$RESOURCE_MONITOR_PID" 2>/dev/null || true wait "$RESOURCE_MONITOR_PID" 2>/dev/null || true fi # Optionally stop Docker container (if we started it) if [ "$STOP_CONTAINER_ON_EXIT" = "true" ] && [ ! -z "$CONTAINER_NAME" ]; then log_info "Stopping Docker container: $CONTAINER_NAME" docker stop "$CONTAINER_NAME" 2>/dev/null || true fi log_success "Cleanup completed" } # Set up signal handlers trap cleanup EXIT trap cleanup INT trap cleanup TERM # Check dependencies check_dependencies() { log_info "Checking dependencies..." # Check if Python is available if ! command -v python3 &> /dev/null; then log_error "Python3 is not installed or not in PATH" exit 1 fi # Check if Docker is available if ! command -v docker &> /dev/null; then log_error "Docker is not installed or not in PATH" exit 1 fi # Check if required Python packages are installed local required_packages=("locust" "psutil" "docker") for package in "${required_packages[@]}"; do if ! python3 -c "import $package" 2>/dev/null; then log_error "Python package '$package' is not installed" log_info "Install with: pip install $package" exit 1 fi done log_success "All dependencies are available" } # Check if application is running check_application() { log_info "Checking if application is accessible at $HOST..." local max_attempts=30 local attempt=1 while [ $attempt -le $max_attempts ]; do if curl -s --max-time 5 "$HOST" > /dev/null; then log_success "Application is accessible" return 0 fi log_info "Attempt $attempt/$max_attempts: Application not yet accessible, waiting..." sleep 2 ((attempt++)) done log_error "Application is not accessible after $max_attempts attempts" return 1 } # Start or check Docker container setup_application() { log_info "Setting up MathBoardAI Agent application..." # Check if container is already running if docker ps --format "table {{.Names}}" | grep -q "^$CONTAINER_NAME$"; then log_success "Container '$CONTAINER_NAME' is already running" return 0 fi # Check if container exists but is stopped if docker ps -a --format "table {{.Names}}" | grep -q "^$CONTAINER_NAME$"; then log_info "Starting existing container '$CONTAINER_NAME'..." docker start "$CONTAINER_NAME" STOP_CONTAINER_ON_EXIT="false" # Don't stop existing containers else log_info "Creating and starting new container '$CONTAINER_NAME'..." # Check if we're in the project directory if [ ! -f "$PROJECT_ROOT/docker-compose.yml" ]; then log_error "docker-compose.yml not found. Please run this script from the project root or ensure the project structure is correct." exit 1 fi # Start with docker-compose cd "$PROJECT_ROOT" docker-compose up -d STOP_CONTAINER_ON_EXIT="true" fi # Wait for application to be ready if ! check_application; then log_error "Failed to start or access the application" exit 1 fi log_success "Application is ready for testing" } # Start resource monitoring start_resource_monitor() { log_info "Starting resource monitor..." # Create results directory if it doesn't exist mkdir -p "$RESULTS_DIR" # Start resource monitor in background python3 "$RESOURCE_MONITOR" \ --container "$CONTAINER_NAME" \ --output "$RESOURCE_LOG" \ --interval 1.0 \ --verbose & RESOURCE_MONITOR_PID=$! # Give it a moment to start sleep 2 # Check if it's running if ! kill -0 "$RESOURCE_MONITOR_PID" 2>/dev/null; then log_error "Failed to start resource monitor" exit 1 fi log_success "Resource monitor started (PID: $RESOURCE_MONITOR_PID)" } # Run Locust load test run_load_test() { log_info "Starting Locust load test..." log_info "Configuration:" log_info " Users: $USERS" log_info " Spawn Rate: $SPAWN_RATE users/second" log_info " Duration: $DURATION seconds" log_info " Host: $HOST" # Create results directory if it doesn't exist mkdir -p "$RESULTS_DIR" # Prepare environment variables for Locust export OPENAI_API_KEY="${OPENAI_API_KEY:-sk-test-key-for-load-testing}" # Run Locust in headless mode cd "$PERFORMANCE_DIR" locust \ -f "$LOCUST_FILE" \ --host="$HOST" \ --users="$USERS" \ --spawn-rate="$SPAWN_RATE" \ --run-time="${DURATION}s" \ --headless \ --csv="$RESULTS_DIR/locust" \ --html="$PERFORMANCE_REPORT" \ --loglevel=INFO & LOCUST_PID=$! # Wait for Locust to complete log_info "Load test running... (PID: $LOCUST_PID)" # Monitor progress local elapsed=0 local report_interval=30 while kill -0 "$LOCUST_PID" 2>/dev/null; do sleep $report_interval elapsed=$((elapsed + report_interval)) local remaining=$((DURATION - elapsed)) if [ $remaining -gt 0 ]; then log_info "Test progress: ${elapsed}s elapsed, ${remaining}s remaining" fi done # Wait for Locust to finish properly wait "$LOCUST_PID" local locust_exit_code=$? if [ $locust_exit_code -eq 0 ]; then log_success "Load test completed successfully" else log_error "Load test failed with exit code: $locust_exit_code" return 1 fi } # Analyze results analyze_results() { log_info "Analyzing test results..." # Check if result files exist local stats_file="$RESULTS_DIR/locust_stats.csv" local failures_file="$RESULTS_DIR/locust_failures.csv" if [ ! -f "$stats_file" ]; then log_error "Locust stats file not found: $stats_file" return 1 fi if [ ! -f "$RESOURCE_LOG" ]; then log_error "Resource log file not found: $RESOURCE_LOG" return 1 fi # Basic analysis using Python python3 - << EOF import csv import sys import os def analyze_locust_results(stats_file, failures_file): """Analyze Locust test results.""" print("=== LOCUST TEST RESULTS ===") # Read stats if os.path.exists(stats_file): with open(stats_file, 'r') as f: reader = csv.DictReader(f) for row in reader: if row['Type'] == 'Aggregated': print(f"Total Requests: {row['Request Count']}") print(f"Failed Requests: {row['Failure Count']}") total_requests = int(row['Request Count']) failed_requests = int(row['Failure Count']) if total_requests > 0: failure_rate = (failed_requests / total_requests) * 100 print(f"Failure Rate: {failure_rate:.2f}%") if failure_rate >= 2.0: print(" FAIL: Failure rate >= 2%") return False else: print(" PASS: Failure rate < 2%") print(f"Average Response Time: {row['Average Response Time']} ms") print(f"95th Percentile: {row['95%']} ms") avg_response = float(row['Average Response Time']) if avg_response >= 15000: # 15 seconds print(" FAIL: Average response time >= 15 seconds") return False else: print(" PASS: Average response time < 15 seconds") break # Read failures if os.path.exists(failures_file): with open(failures_file, 'r') as f: failure_count = sum(1 for line in f) - 1 # Subtract header if failure_count > 0: print(f"Unique Failure Types: {failure_count}") return True def analyze_resource_usage(resource_file): """Analyze resource usage results.""" print("\\n=== RESOURCE USAGE RESULTS ===") memory_values = [] cpu_values = [] with open(resource_file, 'r') as f: reader = csv.DictReader(f) for row in reader: try: memory_mb = float(row['memory_mb']) cpu_percent = float(row['cpu_percent']) memory_values.append(memory_mb) cpu_values.append(cpu_percent) except (ValueError, KeyError): continue if memory_values: avg_memory = sum(memory_values) / len(memory_values) max_memory = max(memory_values) min_memory = min(memory_values) print(f"Memory Usage - Avg: {avg_memory:.1f}MB, Max: {max_memory:.1f}MB, Min: {min_memory:.1f}MB") if max_memory >= 1000: # 1GB print(" FAIL: Memory usage >= 1GB") return False else: print(" PASS: Memory usage < 1GB") if cpu_values: avg_cpu = sum(cpu_values) / len(cpu_values) max_cpu = max(cpu_values) print(f"CPU Usage - Avg: {avg_cpu:.1f}%, Max: {max_cpu:.1f}%") return True # Run analysis stats_file = "$stats_file" failures_file = "$failures_file" resource_file = "$RESOURCE_LOG" locust_pass = analyze_locust_results(stats_file, failures_file) resource_pass = analyze_resource_usage(resource_file) if locust_pass and resource_pass: print("\\n ALL TESTS PASSED!") sys.exit(0) else: print("\\n SOME TESTS FAILED!") sys.exit(1) EOF local analysis_exit_code=$? if [ $analysis_exit_code -eq 0 ]; then log_success "Performance tests passed all acceptance criteria" else log_error "Performance tests failed to meet acceptance criteria" return 1 fi } # Generate summary report generate_report() { log_info "Generating performance test report..." local report_file="$RESULTS_DIR/test_summary.txt" cat > "$report_file" << EOF MathBoardAI Agent - Performance Test Report ======================================= Date: $(date) Duration: ${DURATION} seconds Users: ${USERS} Spawn Rate: ${SPAWN_RATE} users/second Host: ${HOST} Files Generated: - Locust HTML Report: $PERFORMANCE_REPORT - Locust CSV Stats: $RESULTS_DIR/locust_stats.csv - Resource Monitor Log: $RESOURCE_LOG - Full Test Summary: $report_file Test Configuration: - Container Name: $CONTAINER_NAME - Test Script: $LOCUST_FILE - Resource Monitor: $RESOURCE_MONITOR Acceptance Criteria: 50 concurrent users for 3 minutes Failure rate below 2% Average response time below 15 seconds Memory usage below 1GB Resource monitoring data collected For detailed results, check the HTML report and CSV files. EOF log_success "Test report generated: $report_file" # Display key files log_info "Key result files:" log_info " HTML Report: $PERFORMANCE_REPORT" log_info " Resource Log: $RESOURCE_LOG" log_info " Test Summary: $report_file" } # Print usage information show_usage() { cat << EOF Usage: $0 [OPTIONS] Performance Test Orchestration Script for MathBoardAI Agent OPTIONS: -u, --users USERS Number of concurrent users (default: $DEFAULT_USERS) -r, --spawn-rate RATE User spawn rate per second (default: $DEFAULT_SPAWN_RATE) -d, --duration SECONDS Test duration in seconds (default: $DEFAULT_DURATION) -h, --host HOST Target host URL (default: $DEFAULT_HOST) -c, --container NAME Docker container name (default: $DEFAULT_CONTAINER_NAME) --skip-setup Skip application setup (assume already running) --help Show this help message ENVIRONMENT VARIABLES: OPENAI_API_KEY OpenAI API key for testing (default: test key) USERS Same as --users SPAWN_RATE Same as --spawn-rate DURATION Same as --duration HOST Same as --host CONTAINER_NAME Same as --container EXAMPLES: # Run default test (50 users, 3 minutes) $0 # Run with custom parameters $0 --users 100 --duration 300 --spawn-rate 10 # Run against external host $0 --host http://production-server:7860 --skip-setup # Quick test $0 --users 10 --duration 60 RESULTS: Test results are saved to: $RESULTS_DIR/ - locust_stats.csv: Detailed Locust statistics - locust_failures.csv: Failed request details - resource_log.csv: System resource monitoring - performance_report.html: Comprehensive HTML report EOF } # Parse command line arguments parse_args() { while [[ $# -gt 0 ]]; do case $1 in -u|--users) USERS="$2" shift 2 ;; -r|--spawn-rate) SPAWN_RATE="$2" shift 2 ;; -d|--duration) DURATION="$2" shift 2 ;; -h|--host) HOST="$2" shift 2 ;; -c|--container) CONTAINER_NAME="$2" shift 2 ;; --skip-setup) SKIP_SETUP="true" shift ;; --help) show_usage exit 0 ;; *) log_error "Unknown option: $1" show_usage exit 1 ;; esac done } # Main execution main() { log_info "Starting MathBoardAI Agent Performance Test Suite" log_info "==============================================" # Parse arguments parse_args "$@" # Check dependencies check_dependencies # Setup application (unless skipped) if [ "$SKIP_SETUP" != "true" ]; then setup_application else log_info "Skipping application setup" if ! check_application; then log_error "Application is not accessible and setup was skipped" exit 1 fi fi # Start resource monitoring start_resource_monitor # Run load test if ! run_load_test; then log_error "Load test failed" exit 1 fi # Give resource monitor a moment to finish logging sleep 5 # Analyze results if ! analyze_results; then log_error "Performance test analysis failed" exit 1 fi # Generate report generate_report log_success "Performance testing completed successfully!" log_info "Check the results in: $RESULTS_DIR/" } # Run main function with all arguments main "$@"