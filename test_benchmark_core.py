#!/usr/bin/env python3 """ Core test for benchmark system components that don't require external dependencies. This tests the fundamental logic of the benchmark system. """ import sys import os import json import tempfile # Add the tests directory to path sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'tests')) def test_answer_evaluator_comprehensive(): """Comprehensive test of the answer evaluator.""" print("Testing Answer Evaluator (Comprehensive)...") from evaluators.answer_evaluator import evaluate_answer, extract_final_answer # Test numerical extraction extraction_tests = [ ("The answer is 14", 14), ("#### 42", 42), ("Final answer: 3.14", 3.14), ("x = 25", 25), ("Result: -7.5", -7.5), ("No number here", None), ("Multiple numbers 1, 2, 3", 3), # Should take the last one ("Scientific notation 1.5e10", 1.5e10) ] extraction_passed = 0 for text, expected in extraction_tests: result = extract_final_answer(text) if result == expected: print(f" Extraction: '{text[:20]}...' -> {result}") extraction_passed += 1 else: print(f" Extraction: '{text[:20]}...' -> {result} (expected {expected})") # Test full evaluation evaluation_tests = [ ("The answer is 72", "#### 72", True), ("#### 10", "#### 10.0", True), # Should handle int/float equivalence ("Final answer: 5", "#### 5", True), ("I don't know", "#### 42", False), ("The answer is 14.000", "#### 14", True), # Floating point precision ] evaluation_passed = 0 for predicted, ground_truth, expected in evaluation_tests: is_correct, details = evaluate_answer(predicted, ground_truth) if is_correct == expected: print(f" Evaluation: {predicted[:15]}... vs {ground_truth[:15]}... -> {is_correct}") evaluation_passed += 1 else: print(f" Evaluation: {predicted[:15]}... vs {ground_truth[:15]}... -> {is_correct} (expected {expected})") total_tests = len(extraction_tests) + len(evaluation_tests) total_passed = extraction_passed + evaluation_passed print(f" Overall: {total_passed}/{total_tests} tests passed") return total_passed == total_tests def test_dataset_format(): """Test that we can properly parse GSM8K format data.""" print("Testing Dataset Format Parsing...") # Test data in GSM8K format test_problems = [ { "question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?", "answer": "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72" }, { "question": "What is 5 + 3?", "answer": "#### 8" } ] # Create temporary JSONL file with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f: for problem in test_problems: f.write(json.dumps(problem) + '\n') temp_file = f.name try: # Read and parse the file manually (simulate DatasetLoader) problems = [] with open(temp_file, 'r') as f: for i, line in enumerate(f): line = line.strip() if line: problem = json.loads(line) if 'question' in problem and 'answer' in problem: problems.append({ 'id': f"test_{i}", 'question': problem['question'], 'answer': problem['answer'] }) if len(problems) == 2: print(" Parsed correct number of problems") else: print(f" Expected 2 problems, got {len(problems)}") return False if "Natalia" in problems[0]['question']: print(" Problem content parsed correctly") else: print(" Problem content incorrect") return False # Test that we can extract answers from GSM8K format from evaluators.answer_evaluator import extract_final_answer answer1 = extract_final_answer(problems[0]['answer']) answer2 = extract_final_answer(problems[1]['answer']) if answer1 == 72 and answer2 == 8: print(" GSM8K answer extraction works") return True else: print(f" GSM8K extraction failed: got {answer1}, {answer2}") return False finally: os.unlink(temp_file) def test_acceptance_criteria(): """Test the specific acceptance criteria from the task.""" print("Testing Acceptance Criteria...") from evaluators.answer_evaluator import evaluate_answer # Acceptance criterion: correctly identify "The answer is 14" matches ground truth "14" is_correct, details = evaluate_answer("The answer is 14", "14") if is_correct: print(" Acceptance criterion: 'The answer is 14' matches '14'") criterion_1 = True else: print(" Acceptance criterion failed: 'The answer is 14' should match '14'") criterion_1 = False # Test GSM8K format matching gsm8k_answer = "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n#### 72" is_correct, details = evaluate_answer("The answer is 72", gsm8k_answer) if is_correct: print(" GSM8K format matching works") criterion_2 = True else: print(" GSM8K format matching failed") criterion_2 = False return criterion_1 and criterion_2 def test_csv_format(): """Test that we can generate the required CSV format.""" print("Testing CSV Format...") # Simulate benchmark results results_data = [ { 'problem_id': 'gsm8k_0', 'problem_text': 'What is 2 + 2?', 'ground_truth': '#### 4', 'agent_answer': 'The answer is 4', 'agent_correct': True, 'agent_latency_ms': 1500.0, 'agent_verified': True, 'agent_error': '', 'baseline_answer': 'The result is 4', 'baseline_correct': True, 'baseline_latency_ms': 800.0, 'baseline_error': '' } ] # Test that we can create CSV content import csv import io output = io.StringIO() # Get field names from the first result fieldnames = results_data[0].keys() writer = csv.DictWriter(output, fieldnames=fieldnames) writer.writeheader() writer.writerows(results_data) csv_content = output.getvalue() # Check required columns are present required_columns = [ 'problem_id', 'problem_text', 'ground_truth', 'agent_answer', 'agent_correct', 'agent_latency_ms', 'agent_verified', 'baseline_answer', 'baseline_correct', 'baseline_latency_ms' ] all_present = all(col in csv_content for col in required_columns) if all_present: print(" All required CSV columns present") return True else: missing = [col for col in required_columns if col not in csv_content] print(f" Missing CSV columns: {missing}") return False def main(): """Run all core tests.""" print("Testing Benchmark System Core Components") print("=" * 50) tests = [ test_answer_evaluator_comprehensive, test_dataset_format, test_acceptance_criteria, test_csv_format ] passed = 0 total = len(tests) for test_func in tests: try: if test_func(): passed += 1 print(f" {test_func.__name__} PASSED\n") else: print(f" {test_func.__name__} FAILED\n") except Exception as e: print(f" {test_func.__name__} ERROR: {e}\n") import traceback traceback.print_exc() print("=" * 50) print(f"Core Test Results: {passed}/{total} passed") if passed == total: print(" All core tests passed! Benchmark system logic is correct.") print("\nNote: Full system testing requires pandas and openai packages.") print("Install with: pip install pandas openai") return 0 else: print(" Some core tests failed. Please check the implementation.") return 1 if __name__ == "__main__": sys.exit(main())